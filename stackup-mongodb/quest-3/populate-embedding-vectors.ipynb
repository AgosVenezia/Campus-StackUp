{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Populating Embedding Vectors in MongoDB Atlas\n",
    "\n",
    "In this Python notebook, we'll be using the embedding models we've downloaded to our local device to create embedding attributes for our movies dataset. Once we've done that, we'll be using `pymongo` to add these new embedding attributes to our dataset in MongoDB.\n",
    "\n",
    "The cool thing about what we're doing here is that we're generating all the embeddings locally (i.e. no external API calls needed); so if you have your own custom embedding models, you can make use of that for other projects too! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Settings\n",
    "We start by loading up some environment settings. These should all have been configured in Quest 1, so head to Quest 1 if you're missing out anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATLAS_URI Connection string found: mongodb+srv://yongtaufoo:mucjOuDXLysFfEGA@cluster0.ds8hjdi.mongodb.net/?retryWrites=true&w=majority\n"
     ]
    }
   ],
   "source": [
    "# Load settings from .env file\n",
    "import sys\n",
    "from dotenv import find_dotenv, dotenv_values\n",
    "\n",
    "# Change system path to root direcotry\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "# _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "config = dotenv_values(find_dotenv())\n",
    "\n",
    "# For debugging purposes\n",
    "# print (config)\n",
    "\n",
    "ATLAS_URI = config.get('ATLAS_URI')\n",
    "\n",
    "if not ATLAS_URI:\n",
    "    raise Exception (\"'ATLAS_URI' is not set.  Please set it above to continue...\")\n",
    "else:\n",
    "    print(\"ATLAS_URI Connection string found:\", ATLAS_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our variables\n",
    "DB_NAME = 'sample_mflix'\n",
    "COLLECTION_NAME = 'embedded_movies'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize Mongo Atlas Client\n",
    "Then, we intialize a connection to Mongo Atlas Client by using our unique ATLAS_URI value. Once we've succesfully connected, we'll be able to interact with our MongoDB database. For example, in the second code cell, we're getting our embedded_movies dataset and printing out its document count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the Mongo Atlas database!\n"
     ]
    }
   ],
   "source": [
    "from AtlasClient import AtlasClient\n",
    "\n",
    "atlas_client = AtlasClient (ATLAS_URI, DB_NAME)\n",
    "print(\"Connected to the Mongo Atlas database!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document count = 3,483 movies\n"
     ]
    }
   ],
   "source": [
    "collection = atlas_client.get_collection(COLLECTION_NAME)\n",
    "document_count = collection.count_documents({})\n",
    "\n",
    "print (f\"Document count = {document_count:,} movies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generate Embeddings\n",
    "Now for the fun part - we're going to generate all embeddings locally on our computer, using open source models. No API calls or API KEYS needed! ðŸ˜„\n",
    "\n",
    "As mentioned, we'll be using the following models:\n",
    "\n",
    "| model name                              | overall score | model params | model size | embedding length | url                                                            |\n",
    "|-----------------------------------------|---------------|--------------|------------|------------------|----------------------------------------------------------------|\n",
    "| BAAI/bge-small-en-v1.5                  | 62.x          | 33.5 M       | 133 MB     | 384              | https://huggingface.co/BAAI/bge-small-en-v1.5                  |\n",
    "| sentence-transformers/all-mpnet-base-v2 | 57.8          |              | 438 MB     | 768              | https://huggingface.co/sentence-transformers/all-mpnet-base-v2 |\n",
    "| sentence-transformers/all-MiniLM-L6-v2  | 56.x          |              | 91 MB      | 384              | https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# LlamaIndex will download embeddings models as needed\n",
    "# Set llamaindex cache dir to ../cache dir here (Default is system tmp)\n",
    "# This way, we can easily see downloaded artifacts\n",
    "os.environ['LLAMA_INDEX_CACHE_DIR'] = os.path.join(os.path.abspath('../'), 'cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "import time\n",
    "\n",
    "# Helper function to calculate embeddings, given a model\n",
    "def create_embeddings (movies, embedding_model, embedding_attr):\n",
    "    embed_model = HuggingFaceEmbedding(model_name=embedding_model)\n",
    "\n",
    "    t2a = time.perf_counter()\n",
    "    for movie in movies:\n",
    "        movie[embedding_attr] = embed_model.get_text_embedding(movie['plot'])\n",
    "\n",
    "    t2b = time.perf_counter()\n",
    "    # print (f'Embeddings generated for {len(movies):,} movies  in {(t2b-t2a)*1000:,.0f} ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 3,403 from Atlas in 15,173 ms\n"
     ]
    }
   ],
   "source": [
    "# Fetch all movies\n",
    "t1a = time.perf_counter()\n",
    "movies = [m for m in atlas_client.find (collection_name=COLLECTION_NAME, filter={'plot':{\"$exists\": True}}, limit=0)]\n",
    "t1b = time.perf_counter()\n",
    "\n",
    "print (f'Fetched {len(movies):,} from Atlas in {(t1b-t1a)*1000:,.0f} ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding models we want to use\n",
    "model_mappings = {\n",
    "    'BAAI/bge-small-en-v1.5' : {'embedding_attr' : 'plot_embedding_bge_small', 'index_name' : 'idx_plot_embedding_bge_small'},\n",
    "    'sentence-transformers/all-mpnet-base-v2' : {'embedding_attr' : 'plot_embedding_mpnet_base_v2', 'index_name' : 'idx_plot_embedding_mpnet_base_v2'},\n",
    "    'sentence-transformers/all-MiniLM-L6-v2' : {'embedding_attr' : 'plot_embedding_minilm_l6_v2', 'index_name' : 'idx_plot_embedding_minilm_l6_v2'},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------- Embedding Model = BAAI/bge-small-en-v1.5 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/james.lim/anaconda3/envs/atlas-1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model=BAAI/bge-small-en-v1.5, created embeddings for 3,403 movies in 63,485 ms, avg_time_per_movie=19 ms\n",
      "\n",
      "------- Embedding Model = sentence-transformers/all-mpnet-base-v2 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 571/571 [00:00<00:00, 1.53MB/s]\n",
      "pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 438M/438M [00:30<00:00, 14.4MB/s]\n",
      "tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 363/363 [00:00<00:00, 223kB/s]\n",
      "vocab.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232k/232k [00:00<00:00, 8.79MB/s]\n",
      "tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 466k/466k [00:00<00:00, 12.7MB/s]\n",
      "special_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 239/239 [00:00<00:00, 940kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model=sentence-transformers/all-mpnet-base-v2, created embeddings for 3,403 movies in 107,262 ms, avg_time_per_movie=32 ms\n",
      "\n",
      "------- Embedding Model = sentence-transformers/all-MiniLM-L6-v2 ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 612/612 [00:00<00:00, 1.69MB/s]\n",
      "pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90.9M/90.9M [00:05<00:00, 15.4MB/s]\n",
      "tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 350/350 [00:00<00:00, 919kB/s]\n",
      "vocab.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232k/232k [00:00<00:00, 92.8MB/s]\n",
      "tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 466k/466k [00:00<00:00, 17.8MB/s]\n",
      "special_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:00<00:00, 389kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model=sentence-transformers/all-MiniLM-L6-v2, created embeddings for 3,403 movies in 42,667 ms, avg_time_per_movie=13 ms\n"
     ]
    }
   ],
   "source": [
    "# For selected embedding models above, we are going to create vectors for the plot field\n",
    "# Each embedding model will have its own 'plot_embedding' attribute (i.e. we don't want to mix them up)\n",
    "# It might take up to a few minutes to generate the embeddings\n",
    "\n",
    "for key in model_mappings.keys():\n",
    "    embedding_model = key\n",
    "    embedding_attr = model_mappings[key]['embedding_attr']\n",
    "\n",
    "    print (f'\\n------- Embedding Model = {embedding_model} ---------')\n",
    "    t1a = time.perf_counter()\n",
    "    create_embeddings(movies=movies, embedding_model=embedding_model, embedding_attr=embedding_attr)\n",
    "    t1b = time.perf_counter()\n",
    "    avg_time_per_movie = (t1b-t1a)*1000 / len(movies)\n",
    "    print (f'model={embedding_model}, created embeddings for {len(movies):,} movies in {(t1b-t1a)*1000:,.0f} ms, avg_time_per_movie={avg_time_per_movie:,.0f} ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Inspect Generated Embeddings\n",
    "Great job! At this point, we should have succesfully generated 3 sets of embeddings, 1 set for each embedding model we used. Let's try running the code cell below a few times to see the embeddings generated by the different embedding models for a given random movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selected movie:  Sliding Doors\n",
      "Movie plot:  A London woman's love life and career both hinge, unknown to her, on whether or not she catches a train. We see it both ways, in parallel. \n",
      "\n",
      "plot_embeddings (existing openAI generated), len=1536 , [-0.010079535, -0.018171923, 0.017073765, -0.022433829, -0.0016080192]...\n",
      "\n",
      "plot_embedding_bge_small , len=384 , [0.0446619912981987, -0.009976099245250225, 0.043245185166597366, 0.06763069331645966, 0.012619727291166782]...\n",
      "\n",
      "plot_embedding_mpnet_base_v2 , len=768 , [0.011600842699408531, 0.038084715604782104, -0.028071818873286247, -0.001920797978527844, -0.03404993563890457]...\n",
      "\n",
      "plot_embedding_minilm_l6_v2 , len=384 , [-0.02995208464562893, -0.029346900060772896, 0.04203882813453674, 0.0758424922823906, 0.0363309420645237]...\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "movie = random.choice(movies)\n",
    "print ('Randomly selected movie: ', movie['title'])\n",
    "print ('Movie plot: ', movie['plot'], '\\n')\n",
    "print (f'plot_embeddings (existing openAI generated), len={len(movie[\"plot_embedding\"])} , {movie[\"plot_embedding\"][:5]}...\\n')\n",
    "print (f'plot_embedding_bge_small , len={len(movie[\"plot_embedding_bge_small\"])} , {movie[\"plot_embedding_bge_small\"][:5]}...\\n')\n",
    "print (f'plot_embedding_mpnet_base_v2 , len={len(movie[\"plot_embedding_mpnet_base_v2\"])} , {movie[\"plot_embedding_mpnet_base_v2\"][:5]}...\\n')\n",
    "print (f'plot_embedding_minilm_l6_v2 , len={len(movie[\"plot_embedding_minilm_l6_v2\"])} , {movie[\"plot_embedding_minilm_l6_v2\"][:5]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Add Embeddings to MongoDB Atlas\n",
    "Now that we've generated all our embeddings, let's go ahead and add them into our dataset in MongoDB Atlas! We'll be using a bulk update approach to save some time in uploading our embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to update 3403 movies in Atlas...\n",
      "Update matched count: 3403\n",
      "Update modified count: 0\n",
      "Updated 3,403 in Atlas in 24,632 ms\n"
     ]
    }
   ],
   "source": [
    "from pymongo import ReplaceOne\n",
    "\n",
    "collection = atlas_client.get_collection(COLLECTION_NAME)\n",
    "replacements = [ReplaceOne ({\"_id\" : movie[\"_id\"]}, movie) for movie in movies]\n",
    "\n",
    "# Perform bulk replacement\n",
    "print (f'About to update {len(replacements)} movies in Atlas...')\n",
    "t1a = time.perf_counter()\n",
    "result = collection.bulk_write(replacements)\n",
    "t1b = time.perf_counter()\n",
    "\n",
    "# Print result\n",
    "print(f\"Update matched count: {result.matched_count}\")\n",
    "print(f\"Update modified count: {result.modified_count}\")\n",
    "print (f'Updated {len(movies):,} in Atlas in {(t1b-t1a)*1000:,.0f} ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're done with this notebook! Please **head back to the Quest page on StackUp now** where we'll be proceeding with the next step - creating a search index for each of our newly created custom embeddings. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlas-1",
   "language": "python",
   "name": "atlas-1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
